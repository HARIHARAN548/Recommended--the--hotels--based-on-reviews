{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dhari\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dhari\\anaconda3\\lib\\site-packages (from nltk) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\dhari\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\dhari\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\dhari\\anaconda3\\lib\\site-packages (from nltk) (2020.10.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_excel(r'C:\\Users\\Madhu Soundhar\\Downloads\\raw_djubo__new22092020.xlsx') #load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>reviewid</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>strength</th>\n",
       "      <th>polarity</th>\n",
       "      <th>properties</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the whole stay was good and comfortable wit...</td>\n",
       "      <td>142332320200919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perfect and cozy stay for couples. Parking fac...</td>\n",
       "      <td>142332420200919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peaceful place and very friendly staff. Food i...</td>\n",
       "      <td>142344120170703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Had a great stay here with all safety measures...</td>\n",
       "      <td>142344220200829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most memorable stays ..must visit ..guys plan ...</td>\n",
       "      <td>142344320181028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content         reviewid  \\\n",
       "0  On the whole stay was good and comfortable wit...  142332320200919   \n",
       "1  Perfect and cozy stay for couples. Parking fac...  142332420200919   \n",
       "2  Peaceful place and very friendly staff. Food i...  142344120170703   \n",
       "3  Had a great stay here with all safety measures...  142344220200829   \n",
       "4  Most memorable stays ..must visit ..guys plan ...  142344320181028   \n",
       "\n",
       "   sentiment_score  strength  polarity  properties  \n",
       "0              NaN       NaN       NaN         NaN  \n",
       "1              NaN       NaN       NaN         NaN  \n",
       "2              NaN       NaN       NaN         NaN  \n",
       "3              NaN       NaN       NaN         NaN  \n",
       "4              NaN       NaN       NaN         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6815 entries, 0 to 6814\n",
      "Data columns (total 6 columns):\n",
      "review_content     6815 non-null object\n",
      "reviewid           6815 non-null int64\n",
      "sentiment_score    0 non-null float64\n",
      "strength           0 non-null float64\n",
      "polarity           0 non-null float64\n",
      "properties         0 non-null float64\n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 319.5+ KB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_content', 'reviewid', 'sentiment_score', 'strength', 'polarity',\n",
       "       'properties'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langid in c:\\users\\madhu soundhar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\madhu soundhar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from langid) (1.16.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid # language identification (i.e. what language is this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.textcat import TextCat   # language identification from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in c:\\users\\madhu soundhar\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2020.11.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package crubadan to C:\\Users\\Madhu\n",
      "[nltk_data]     Soundhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\crubadan.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('crubadan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextCat() # N-Gram-Based Text Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhu Soundhar\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "reviews1=reviews.drop(reviews.ix[:, 'reviewid':'properties'].columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On the whole stay was good and comfortable wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perfect and cozy stay for couples. Parking fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peaceful place and very friendly staff. Food i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Had a great stay here with all safety measures...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most memorable stays ..must visit ..guys plan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content\n",
       "0  On the whole stay was good and comfortable wit...\n",
       "1  Perfect and cozy stay for couples. Parking fac...\n",
       "2  Peaceful place and very friendly staff. Food i...\n",
       "3  Had a great stay here with all safety measures...\n",
       "4  Most memorable stays ..must visit ..guys plan ..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           (en, -256.4077625274658)\n",
       "1            (en, -328.434006690979)\n",
       "2          (en, -211.29365396499634)\n",
       "3           (en, -796.6728827953339)\n",
       "4            (en, -98.2474455833435)\n",
       "5       (en, -0.0004711151123046875)\n",
       "6          (en, -444.70057344436646)\n",
       "7          (en, -1324.3847527503967)\n",
       "8          (en, -24.584232807159424)\n",
       "9           (en, -59.71237134933472)\n",
       "10           (en, -8.61999797821045)\n",
       "11          (en, -51.61979699134827)\n",
       "12          (en, -270.1850390434265)\n",
       "13         (en, -160.54700207710266)\n",
       "14          (en, -656.5073373317719)\n",
       "15          (en, -63.72074365615845)\n",
       "16          (en, -99.10531187057495)\n",
       "17          (en, -682.8472075462341)\n",
       "18          (en, -459.8219509124756)\n",
       "19          (en, -848.4858906269073)\n",
       "20          (en, -660.7894186973572)\n",
       "21          (en, -220.7600646018982)\n",
       "22         (en, -111.70770263671875)\n",
       "23          (en, -504.9403910636902)\n",
       "24         (en, -1364.7697110176086)\n",
       "25          (en, -808.6870522499084)\n",
       "26          (en, -688.7992253303528)\n",
       "27           (en, 9.061840057373047)\n",
       "28         (en, -15.135066509246826)\n",
       "29           (es, 2.864696502685547)\n",
       "                    ...             \n",
       "5970       (en, -2612.1559686660767)\n",
       "5971       (en, -1516.5238490104675)\n",
       "5972        (en, -839.8062176704407)\n",
       "5973        (en, -770.3926525115967)\n",
       "5974        (en, -600.4243927001953)\n",
       "5975       (en, -2100.7926831245422)\n",
       "5976        (en, -2250.241047143936)\n",
       "5977        (en, -421.1348741054535)\n",
       "5978        (en, -690.1804730892181)\n",
       "5979       (en, -482.60138964653015)\n",
       "5980       (en, -1215.6520442962646)\n",
       "5981       (en, -2674.5173823833466)\n",
       "5982         (en, -995.130687713623)\n",
       "5983       (en, -61.951860427856445)\n",
       "5984       (en, -1786.1416165828705)\n",
       "5985       (en, -404.60324335098267)\n",
       "5986        (en, -4896.587152957916)\n",
       "5987       (en, -128.41312909126282)\n",
       "5988       (en, -1742.9656512737274)\n",
       "5989       (en, -20.880557537078857)\n",
       "5990        (es, -309.7973644733429)\n",
       "5991        (en, -710.1081428527832)\n",
       "5992        (en, -380.6067228317261)\n",
       "5993       (en, -128.41312909126282)\n",
       "5994        (en, -2775.832308769226)\n",
       "5995       (en, -2541.6399431228638)\n",
       "5996       (en, -1360.1531496047974)\n",
       "5997       (en, -1932.4164667129517)\n",
       "5998       (en, -1622.9665009975433)\n",
       "5999        (en, -760.4897553920746)\n",
       "Name: review_content, Length: 6000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summerize the labelled language\n",
    "reviews1['review_content'][0:6000].apply(langid.classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Madhu\n",
      "[nltk_data]     Soundhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    eng\n",
       "1    eng\n",
       "2    eng\n",
       "3    eng\n",
       "4    eng\n",
       "5    eng\n",
       "Name: review_content, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to identify the languages of the first five tweets again\n",
    "reviews1['review_content'][0:6].apply(tc.guess_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the language id for each text\n",
    "ids_langid = reviews1['review_content'].apply(langid.classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the language label\n",
    "langs = ids_langid.apply(lambda tuple: tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tagged languages (estimated):\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# how many unique language labels were applied?\n",
    "print(\"Number of tagged languages (estimated):\")\n",
    "print(len(langs.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of data in English (estimated):\n",
      "96.62509170946441\n"
     ]
    }
   ],
   "source": [
    "# percent of the total dataset in English\n",
    "print(\"Percent of data in English (estimated):\")\n",
    "print((sum(langs==\"en\")/len(langs))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our list of languages to a dataframe\n",
    "langs_df = pd.DataFrame(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en    6585\n",
      "de      69\n",
      "fr      34\n",
      "it      31\n",
      "es      26\n",
      "nl      23\n",
      "da       9\n",
      "hi       4\n",
      "id       3\n",
      "sv       3\n",
      "ca       2\n",
      "af       2\n",
      "ms       2\n",
      "rw       2\n",
      "pl       2\n",
      "no       2\n",
      "ar       2\n",
      "ja       2\n",
      "pt       2\n",
      "fi       1\n",
      "sl       1\n",
      "oc       1\n",
      "mt       1\n",
      "eu       1\n",
      "gl       1\n",
      "et       1\n",
      "he       1\n",
      "nb       1\n",
      "cs       1\n",
      "Name: review_content, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the number of times we see each language\n",
    "langs_count = langs_df.review_content.value_counts()\n",
    "print(langs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'C:\\Users\\Madhu Soundhar\\Downloads\\raw_djubo__new22092020.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"review_content\"] == \"\"] #Checks for empty review strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'No Negative' or 'No Positive' from text\n",
    "df[\"review_content\"] = df[\"review_content\"].apply(lambda x: x.replace(\"No Negative\", \"\").return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user doesn't leave any negative feedback comment, this will appear as \"No Negative\" in our data. This is\n",
    "the same for the positive comments with the default value \"No Positive\". We have to remove those parts from\n",
    "our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    " # lower text\n",
    " text = text.lower()\n",
    " # tokenize text and remove puncutation\n",
    " text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    " # remove words that contain numbers\n",
    " text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    " # remove stop words\n",
    " stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    " # remove empty tokens\n",
    " text = [t for t in text if len(t) > 0]\n",
    " # pos tag text\n",
    " pos_tags = pos_tag(text)\n",
    " # lemmatize text\n",
    " text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    " # remove words with only one letter\n",
    " text = [t for t in text if len(t) > 1]\n",
    " # join all\n",
    " text = \" \".join(text)\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean textual data, here i used theese all functions\n",
    "lower the text , tokenize the text (split the text into words) and remove the punctuation, remove useless words\n",
    "that contain numbers, remove useless stop words like 'the', 'a' ,'this' etc, Part-Of-Speech (POS) tagging: assign\n",
    "a tag to every word to define if it corresponds to a noun, a verb etc. using the WordNet lexical database,\n",
    "lemmatize the text: transform every word into their root form (e.g. rooms -> room, slept -> sleep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"] = df[\"review_content\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "df[\"sentiments\"] = df[\"review_content\"].apply(lambda x: sid.polarity_scores(x))\n",
    "df = pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i take sentiment analysis features because we can guess that customers reviews are highly linked to how\n",
    "they felt about their stay at the hotel. Here We use Vader, which is a part of the NLTK module designed for\n",
    "sentiment analysis. Vader uses a lexicon of words to find which ones are positives or negatives. It also takes\n",
    "into accout the context of the sentences to determine the sentiment scores.For each text, Vader retuns 4\n",
    "values. A neutrality score, a positivity score, a negativity score,an overall score that summarizes the previous\n",
    "scores,We will integrate those 4 values as features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of characters column\n",
    "df[\"nb_chars\"] = df[\"review_content\"].apply(lambda x: len(x))\n",
    "# add number of words column\n",
    "df[\"nb_words\"] = df[\"review_content\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc2vec vector columns\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"clean\"].apply(lambda x: x\n",
    "# train a Doc2Vec model with our text data\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
    "# transform each document into a vector data\n",
    "doc2vec_df = df[\"clean\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
    "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
    "reviews_df = pd.concat([df, doc2vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next step consist in extracting vector representations for every review. The module Gensim creates a\n",
    "numerical vector representation of every word in the corpus by using the contexts in which they appear\n",
    "(Word2Vec). This is performed using shallow neural networks. What's interesting is that similar words will have\n",
    "similar representation vectors.\n",
    "Each text can also be transformed into numerical vectors using the word vectors (Doc2Vec). Same texts will\n",
    "also have similar representations and that is why we can use those vectors as training features.\n",
    "We first have to train a Doc2Vec model by feeding in our text data. By applying this model on our reviews, we\n",
    "can get those representation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tf-idfs columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 10)\n",
    "tfidf_result = tfidf.fit_transform(df[\"clean\"]).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "tfidf_df.index = df.index\n",
    "df = pd.concat([df, tfidf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud function\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    " background_color = 'white',\n",
    " max_words = 200,\n",
    " max_font_size = 40,\n",
    " scale = 3,\n",
    " random_state = 42\n",
    " ).generate(str(data))\n",
    "    fig = plt.figure(1, figsize = (20, 20))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.show()\n",
    "\n",
    "# print wordcloud\n",
    "show_wordcloud(df[\"review_content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the words are indeed related to the hotels: room, staff, breakfast, etc. Some words are more related to\n",
    "the customer experience with the hotel stay: perfect, loved, expensive, dislike, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest positive sentiment reviews (with more than 5 words)\n",
    "df[df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"review_content\", \"pos\"]].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest negative sentiment reviews (with more than 5 words)\n",
    "df[df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"review_content\", \"neg\"]].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sentiment distribution for positive and negative reviews\n",
    "import seaborn as sns\n",
    "for x in [0, 1]:\n",
    "    subset = df[df['neg'] == x]\n",
    "\n",
    " # Draw the density plot\n",
    " if x == 0:\n",
    "        label = \"Good reviews\"\n",
    "        else:\n",
    "            label = \"Bad reviews\"\n",
    "            sns.distplot(subset['compound'], hist = False, label = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is completely possible to use only raw text as input for making predictions. The most important thing is to be\n",
    "able to extract the relevant features from this raw source of data. This kind of data can often come as a good\n",
    "complementary source in data science projects in order to extract more learning features and increase the\n",
    "predictive power of the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
